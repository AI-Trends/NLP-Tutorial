{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_imdb.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaquwIipShI8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a309a35c-008f-4d66-d19c-69df00c6e253"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HEjNZz7PYLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install tensorflow keras gensim scikit-learn\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import FastText"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkSd6moNPnmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(vocab_size,max_len):\n",
        "    \"\"\"\n",
        "        Loads the keras imdb dataset\n",
        "\n",
        "        Args:\n",
        "            vocab_size = {int} the size of the vocabulary\n",
        "            max_len = {int} the maximum length of input considered for padding\n",
        "\n",
        "        Returns:\n",
        "            X_train = tokenized train data\n",
        "            X_test = tokenized test data\n",
        "\n",
        "    \"\"\"\n",
        "    INDEX_FROM = 3\n",
        "\n",
        "    # save np.load\n",
        "    np_load_old = np.load\n",
        "\n",
        "    # modify the default parameters of np.load\n",
        "    np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True)\n",
        "\n",
        "    (X_train,y_train),(X_test,y_test) = imdb.load_data(num_words = vocab_size,index_from = INDEX_FROM)\n",
        "\n",
        "    # restore np.load for future normal usage\n",
        "    np.load = np_load_old\n",
        "\n",
        "    print(len(X_train), len(X_test), len(y_train), len(y_test), \"#####################################\")\n",
        "\n",
        "    return X_train,X_test,y_train,y_test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pukDaxHTPnrk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data_for_word_vectors_imdb(X_train):\n",
        "    \"\"\"\n",
        "        Prepares the input\n",
        "\n",
        "        Args:\n",
        "            X_train = tokenized train data\n",
        "\n",
        "        Returns:\n",
        "            sentences = {list} sentences containing words as tokens\n",
        "            word_index = {dict} word and its indexes in whole of imdb corpus\n",
        "\n",
        "    \"\"\"\n",
        "    INDEX_FROM = 3\n",
        "    word_to_index = imdb.get_word_index()\n",
        "    word_to_index = {k:(v+INDEX_FROM) for k,v in word_to_index.items()}\n",
        "    word_to_index[\"<START>\"] =1\n",
        "    word_to_index[\"<UNK>\"]=2\n",
        "\n",
        "    index_to_word = {v:k for k,v in word_to_index.items()}\n",
        "\n",
        "    sentences = []\n",
        "    for i in range(len(X_train)):\n",
        "        temp = [index_to_word[ids] for ids in X_train[i]]\n",
        "        sentences.append(temp)\n",
        "    \"\"\"\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(sentences)\n",
        "    word_indexes = tokenizer.word_index\n",
        "    \"\"\"\n",
        "\n",
        "    #print(sentences[:10],word_to_index,\"sentences[:10],word_to_index[:10]*********************************************\")\n",
        "    return sentences,word_to_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilklcXU3Pnzp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def building_word_vector_model(option,sentences,embed_dim,window):\n",
        "    \"\"\"\n",
        "        Builds the word vector\n",
        "\n",
        "        Args:\n",
        "            option = {bool} 0 for Word2vec. 1 for gensim Fastext. 2 for Fasttext 2018.\n",
        "            sentences = {list} list of tokenized words\n",
        "            embed_dim = {int} embedding dimension of the word vectors\n",
        "            window = {int} max distance between current and predicted word\n",
        "\n",
        "        Returns:\n",
        "            model = Word2vec/Gensim fastText/ Fastext_2018 model trained on the training corpus\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    if option == 0:\n",
        "        print(\"Training a word2vec model\")\n",
        "        model = Word2Vec(sentences=sentences, size = embed_dim, window = window) \n",
        "        print(\"Training complete\")\n",
        "\n",
        "    elif option == 1:\n",
        "        print(\"Training a Gensim FastText model\")\n",
        "        model = FastText(sentences=sentences, size = embed_dim, window = window) # workers = workers, \n",
        "        print(\"Training complete\")\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCle-bBtPn5s",
        "colab_type": "code",
        "outputId": "2e809e3d-9dfa-4080-89dd-8b2ca4081871",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# specify “option” as  0 – Word2vec, 1 – FastText\n",
        "\n",
        "option = 1\n",
        "\n",
        "embed_dim = 300\n",
        "max_len= 200\n",
        "vocab_size= 1000\n",
        "window = 1\n",
        "\n",
        "\n",
        "x_train,x_test,y_train,y_test = load_data(vocab_size,max_len)\n",
        "sentences,word_ix = prepare_data_for_word_vectors_imdb(x_train)\n",
        "model_wv = building_word_vector_model(option,sentences,embed_dim, window)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000 25000 25000 25000 #####################################\n",
            "Training a Gensim FastText model\n",
            "Training complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtM_jLWxQzYV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8405a228-ca50-431d-ff69-86e959ace57e"
      },
      "source": [
        "x_train.shape, y_train.shape, len(x_train[0]), y_train[0], x_train[0]"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((25000,),\n",
              " (25000,),\n",
              " 218,\n",
              " 1,\n",
              " [1,\n",
              "  14,\n",
              "  22,\n",
              "  16,\n",
              "  43,\n",
              "  530,\n",
              "  973,\n",
              "  2,\n",
              "  2,\n",
              "  65,\n",
              "  458,\n",
              "  2,\n",
              "  66,\n",
              "  2,\n",
              "  4,\n",
              "  173,\n",
              "  36,\n",
              "  256,\n",
              "  5,\n",
              "  25,\n",
              "  100,\n",
              "  43,\n",
              "  838,\n",
              "  112,\n",
              "  50,\n",
              "  670,\n",
              "  2,\n",
              "  9,\n",
              "  35,\n",
              "  480,\n",
              "  284,\n",
              "  5,\n",
              "  150,\n",
              "  4,\n",
              "  172,\n",
              "  112,\n",
              "  167,\n",
              "  2,\n",
              "  336,\n",
              "  385,\n",
              "  39,\n",
              "  4,\n",
              "  172,\n",
              "  2,\n",
              "  2,\n",
              "  17,\n",
              "  546,\n",
              "  38,\n",
              "  13,\n",
              "  447,\n",
              "  4,\n",
              "  192,\n",
              "  50,\n",
              "  16,\n",
              "  6,\n",
              "  147,\n",
              "  2,\n",
              "  19,\n",
              "  14,\n",
              "  22,\n",
              "  4,\n",
              "  2,\n",
              "  2,\n",
              "  469,\n",
              "  4,\n",
              "  22,\n",
              "  71,\n",
              "  87,\n",
              "  12,\n",
              "  16,\n",
              "  43,\n",
              "  530,\n",
              "  38,\n",
              "  76,\n",
              "  15,\n",
              "  13,\n",
              "  2,\n",
              "  4,\n",
              "  22,\n",
              "  17,\n",
              "  515,\n",
              "  17,\n",
              "  12,\n",
              "  16,\n",
              "  626,\n",
              "  18,\n",
              "  2,\n",
              "  5,\n",
              "  62,\n",
              "  386,\n",
              "  12,\n",
              "  8,\n",
              "  316,\n",
              "  8,\n",
              "  106,\n",
              "  5,\n",
              "  4,\n",
              "  2,\n",
              "  2,\n",
              "  16,\n",
              "  480,\n",
              "  66,\n",
              "  2,\n",
              "  33,\n",
              "  4,\n",
              "  130,\n",
              "  12,\n",
              "  16,\n",
              "  38,\n",
              "  619,\n",
              "  5,\n",
              "  25,\n",
              "  124,\n",
              "  51,\n",
              "  36,\n",
              "  135,\n",
              "  48,\n",
              "  25,\n",
              "  2,\n",
              "  33,\n",
              "  6,\n",
              "  22,\n",
              "  12,\n",
              "  215,\n",
              "  28,\n",
              "  77,\n",
              "  52,\n",
              "  5,\n",
              "  14,\n",
              "  407,\n",
              "  16,\n",
              "  82,\n",
              "  2,\n",
              "  8,\n",
              "  4,\n",
              "  107,\n",
              "  117,\n",
              "  2,\n",
              "  15,\n",
              "  256,\n",
              "  4,\n",
              "  2,\n",
              "  7,\n",
              "  2,\n",
              "  5,\n",
              "  723,\n",
              "  36,\n",
              "  71,\n",
              "  43,\n",
              "  530,\n",
              "  476,\n",
              "  26,\n",
              "  400,\n",
              "  317,\n",
              "  46,\n",
              "  7,\n",
              "  4,\n",
              "  2,\n",
              "  2,\n",
              "  13,\n",
              "  104,\n",
              "  88,\n",
              "  4,\n",
              "  381,\n",
              "  15,\n",
              "  297,\n",
              "  98,\n",
              "  32,\n",
              "  2,\n",
              "  56,\n",
              "  26,\n",
              "  141,\n",
              "  6,\n",
              "  194,\n",
              "  2,\n",
              "  18,\n",
              "  4,\n",
              "  226,\n",
              "  22,\n",
              "  21,\n",
              "  134,\n",
              "  476,\n",
              "  26,\n",
              "  480,\n",
              "  5,\n",
              "  144,\n",
              "  30,\n",
              "  2,\n",
              "  18,\n",
              "  51,\n",
              "  36,\n",
              "  28,\n",
              "  224,\n",
              "  92,\n",
              "  25,\n",
              "  104,\n",
              "  4,\n",
              "  226,\n",
              "  65,\n",
              "  16,\n",
              "  38,\n",
              "  2,\n",
              "  88,\n",
              "  12,\n",
              "  16,\n",
              "  283,\n",
              "  5,\n",
              "  16,\n",
              "  2,\n",
              "  113,\n",
              "  103,\n",
              "  32,\n",
              "  15,\n",
              "  16,\n",
              "  2,\n",
              "  19,\n",
              "  178,\n",
              "  32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0knY1YTUXpFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def padding_input(X_train,X_test,maxlen):\n",
        "    \"\"\"\n",
        "        Pads the input upto considered max length\n",
        "\n",
        "        Args:\n",
        "            X_train = tokenized train data\n",
        "            X_test = tokenized test data\n",
        "\n",
        "        Returns:\n",
        "            X_train_pad = padded tokenized train data\n",
        "            X_test_pad = padded tokenized test data\n",
        "\n",
        "    \"\"\"\n",
        "    print(X_train.shape, X_test.shape, \"before padding\")\n",
        "\n",
        "    X_train_pad = pad_sequences(X_train,maxlen=maxlen,padding=\"post\")\n",
        "\n",
        "    X_test_pad = pad_sequences(X_test,maxlen=maxlen,padding=\"post\")\n",
        "\n",
        "    print(X_train_pad.shape, X_test_pad.shape, \"after padding\")\n",
        "\n",
        "    return X_train_pad,X_test_pad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrgsBB2vPn_0",
        "colab_type": "code",
        "outputId": "f7ad3da7-7f19-4d40-ec82-d3494474489e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "x_train_pad,x_test_pad = padding_input(x_train,x_test,max_len)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000,) (25000,) before padding\n",
            "(25000, 200) (25000, 200) after padding\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXNkqVuIP33I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "013f489f-7ced-417a-cebe-63ba41bf6f19"
      },
      "source": [
        "embedding_matrix = np.zeros((vocab_size,embed_dim))\n",
        "\n",
        "for word, i in word_ix.items():\n",
        "    try:\n",
        "        embedding_vector = w2vmodel[word]\n",
        "        \n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i]=embedding_vector\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print(embedding_matrix.shape ,\"embedding_matrix\")"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 300) embedding_matrix\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywnNZLlOPtYk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "8a3649d7-e99d-4367-88a8-2b08c96c6ae6"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import Flatten\n",
        "from keras.initializers import Constant\n",
        "\n",
        "print('Training model.')\n",
        "\n",
        "# define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size,\n",
        "                            embed_dim,\n",
        "                            embeddings_initializer=Constant(embedding_matrix),\n",
        "                            input_length=max_len,\n",
        "                            trainable=False))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='tanh'))\n",
        "model.add(Dense(256, activation='tanh'))\n",
        "model.add(Dense(128, activation='tanh'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['acc'])\n",
        "\n",
        "# summarize the model\n",
        "print(model.summary())\n",
        "\n",
        "model.fit(x_train_pad,y_train,\n",
        "          batch_size=2048,\n",
        "          epochs=1,\n",
        "          validation_data=(x_test_pad,y_test))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model.\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 200, 300)          300000    \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 60000)             0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 512)               30720512  \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 31,184,865\n",
            "Trainable params: 30,884,865\n",
            "Non-trainable params: 300,000\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/1\n",
            "25000/25000 [==============================] - 81s 3ms/step - loss: 0.6932 - acc: 0.5066 - val_loss: 0.6935 - val_acc: 0.5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8dd0b59e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgbsnlQHZ6Up",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "# print('Accuracy: %f' % (accuracy))\n",
        "# print('Loss: %f' % (loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_jMSTdcVBQg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "88686e8f-7410-44d6-de6b-fdec7012dc6b"
      },
      "source": [
        "model.predict(x_test_pad[6].reshape(1, x_test_pad.shape[1]))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.51251537]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w96ZnKAVJZG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d398c1d6-2920-4047-efd6-56c1e1d39819"
      },
      "source": [
        "y_test[6]"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBtJl0niYN9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}